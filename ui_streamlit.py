import asyncio
import html
import logging
import time
import uuid
import streamlit as st
import streamlit.components.v1 as components

import utils

# ä½ ç°æœ‰çš„æ„å»ºå‡½æ•°ï¼ˆæŒ‰ä½ çš„é¡¹ç›®æ”¹ importï¼‰
from llm import build_llm, build_reflection_multi_agent_graph
from tools import TOOL_REGISTRY
from trace_visualizer import generate_mermaid_sequence
from mcp_adapter import CLIENT_MANAGER

import os, gc, asyncio, tracemalloc
import psutil

tracemalloc.start(25)
_proc = psutil.Process(os.getpid())

def mem_snapshot(tag: str):
    rss = _proc.memory_info().rss / (1024 * 1024)
    cur, peak = tracemalloc.get_traced_memory()
    cur /= 1024 * 1024
    peak /= 1024 * 1024
    try:
        loop = asyncio.get_running_loop()
        tasks = len(asyncio.all_tasks(loop))
    except Exception:
        tasks = -1
    print(f"[MEM] {tag} rss={rss:.1f}MB py_cur={cur:.1f}MB py_peak={peak:.1f}MB gc={gc.get_count()} tasks={tasks}")

def render_mermaid(code: str, height=600):
    """
    ä½¿ç”¨ HTML/JS æ¸²æŸ“ Mermaid å›¾è¡¨
    """
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
        <script>
            mermaid.initialize({{ startOnLoad: true, theme: 'default' }});
        </script>
    </head>
    <body>
        <div class="mermaid">
            {code}
        </div>
    </body>
    </html>
    """
    components.html(html_code, height=height, scrolling=True)


def reconstruct_state_from_traces(traces):
    """ä» traces ä¸­é‡å»ºå½“å‰çŠ¶æ€"""
    current_state = {
        "plan": None,
        "step_idx": 0,
        "artifacts": {},
        # è®°å½•åŒä¸€æ­¥éª¤çš„å¤šæ¬¡äº§å‡ºï¼ˆé‡è¯•/å·¥å…·è¿”å›/å¤šè½®å¯¹è¯ï¼‰
        # step_id -> [ {"trace_i": int, "node": str, "artifact": dict} ]
        "artifact_history": {},
        "step_failures": {},
        "step_tool_stats": {},
        "no_progress": {},
        "last_feedback": {},
        "executed_steps": [],
        "pending_steps": [],
        "last_step_id": None,
        "last_agent_role": None,
        "iter_count": 0,
        "done": False,
    }

    # æŒ‰æ—¶é—´é¡ºåºå¤„ç† tracesï¼ˆä»æ—§åˆ°æ–°ï¼‰ï¼Œæ¨¡æ‹ŸçŠ¶æ€æ›´æ–°
    for trace_i, trace in enumerate(traces):
        patch = trace.get("patch") or {}
        node = trace.get("node", "")

        # è°ƒè¯•ï¼šæ‰“å°èŠ‚ç‚¹å’Œè¡¥ä¸ç»“æ„
        # print(f"èŠ‚ç‚¹: {node}, è¡¥ä¸é”®: {list(patch.keys())}")

        # åº”ç”¨è¡¥ä¸åˆ°å½“å‰çŠ¶æ€
        # LangGraph çš„è¡¥ä¸å¯èƒ½æ˜¯åµŒå¥—çš„ï¼Œä¾‹å¦‚ {"plan": {...}} æˆ– {"step_idx": 1}
        # ä½†ä¹Ÿå¯èƒ½æ˜¯æ›´å¤æ‚çš„ç»“æ„ï¼Œä¾‹å¦‚ {"artifacts": {"step1": {...}}}
        for key, value in patch.items():
            if key in [
                "plan",
                "step_idx",
                "iter_count",
                "done",
                "executed_steps",
                "pending_steps",
                "last_step_id",
                "last_agent_role",
            ]:
                current_state[key] = value
            elif key in [
                "artifacts",
                "step_failures",
                "step_tool_stats",
                "no_progress",
                "last_feedback",
            ]:
                if isinstance(value, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶æ›´æ–°
                    if key == "artifacts":
                        # artifacts å¯èƒ½åŒ…å«å¤šä¸ªæ­¥éª¤çš„è¾“å‡ºï¼Œéœ€è¦åˆå¹¶
                        for step_id, artifact in value.items():
                            current_state[key][step_id] = artifact
                            # åŒæ­¥ç´¯è®¡å†å²ï¼Œä¾¿äº UI å±•ç¤ºâ€œçœ‹èµ·æ¥æ‰§è¡Œäº†å¾ˆå¤šè½®â€
                            if step_id not in current_state["artifact_history"]:
                                current_state["artifact_history"][step_id] = []
                            current_state["artifact_history"][step_id].append(
                                {
                                    "trace_i": trace_i,
                                    "node": node,
                                    "artifact": artifact,
                                }
                            )
                    else:
                        current_state[key].update(value)

    return current_state


def _safe_repr(val, max_len: int = 200) -> str:
    try:
        s = repr(val)
    except Exception:
        s = f"<{type(val).__name__} repr_failed>"
    if max_len and len(s) > max_len:
        return s[:max_len] + "â€¦"
    return s


def _normalize_steps(steps):
    """Normalize plan steps to `list[dict]` for UI rendering.

    Upstream plan generation may occasionally return malformed structures
    (e.g., steps is a string, or step items are not dict-like). The UI should
    be resilient and never crash while rendering.
    """

    if steps is None:
        return []

    # Single step dict
    if isinstance(steps, dict):
        return [steps]

    # A common failure mode: steps accidentally becomes a string
    if isinstance(steps, str):
        logging.warning(
            "ui.normalize_steps: steps is str; coercing to single step | steps=%s",
            _safe_repr(steps),
        )
        return [{"id": "step1", "agent": "æœªçŸ¥", "task": steps}]

    # Anything else that isn't a list/tuple: wrap as a single step
    if not isinstance(steps, (list, tuple)):
        logging.warning(
            "ui.normalize_steps: steps is %s; coercing to single step | steps=%s",
            type(steps).__name__,
            _safe_repr(steps),
        )
        return [{"id": "step1", "agent": "æœªçŸ¥", "task": str(steps)}]

    out = []
    for i, s in enumerate(steps):
        if isinstance(s, dict):
            out.append(s)
            continue
        if hasattr(s, "model_dump"):
            try:
                out.append(s.model_dump())
                continue
            except Exception:
                # fall through to coercion
                pass

        logging.warning(
            "ui.normalize_steps: step[%s] is %s; coercing to dict | step=%s",
            i,
            type(s).__name__,
            _safe_repr(s),
        )
        out.append({"id": f"step{i+1}", "agent": "æœªçŸ¥", "task": str(s)})

    return out


def _escape_md(s: str) -> str:
    return html.escape(str(s or ""))


def _render_step_details_html(
    *,
    step_idx: int,
    step: dict,
    artifact: dict,
    history: list,
    status: str,
    failure_count: int,
    no_progress: bool,
    feedback: dict,
    max_output_chars: int = 8000,
) -> str:
    """Render a single step as <details> HTML block."""
    step_id = str(step.get("id") or f"step{step_idx + 1}")
    agent = str(step.get("agent") or "æœªçŸ¥")
    title = str(step.get("title") or "")
    task = str(step.get("task") or "")
    acceptance = str(step.get("acceptance") or "")

    out = ""
    attempt = None
    tool_calls_count = None
    if isinstance(artifact, dict) and artifact:
        out = str(artifact.get("content") or "")
        attempt = artifact.get("attempt")
        tool_calls_count = artifact.get("tool_calls_count")
        # allow artifact to carry task/acceptance too
        task = task or str(artifact.get("task") or "")
        acceptance = acceptance or str(artifact.get("acceptance") or "")

    is_open = status in ("ğŸ”„", "â³")  # current/in-progress
    header = f"{status} {step_id} ({agent})" + (f" â€” {title}" if title else "")
    if failure_count:
        header += f" | retry={failure_count}"
    if no_progress:
        header += " | no_progress=1"
    if tool_calls_count is not None:
        header += f" | tool_calls={tool_calls_count}"
    if attempt is not None:
        header += f" | attempt={attempt}"
    if history:
        header += f" | updates={len(history)}"

    out_full_len = len(out)
    out_disp = out
    truncated = False
    if max_output_chars and out_full_len > max_output_chars:
        out_disp = out[:max_output_chars]
        truncated = True

    fb_reason = ""
    fb_required = ""
    if isinstance(feedback, dict) and feedback:
        fb_reason = str(feedback.get("reason") or "")
        req = feedback.get("required_changes")
        fb_required = (
            "\n".join([str(x) for x in req])
            if isinstance(req, list)
            else str(req or "")
        )

    parts = []
    parts.append(f"<details {'open' if is_open else ''}>")
    parts.append(f"<summary>{_escape_md(header)}</summary>")
    parts.append("<div style='margin-top: 8px;'>")

    if task:
        parts.append(f"<div><b>ä»»åŠ¡</b><pre style='white-space:pre-wrap'>{_escape_md(task)}</pre></div>")
    if acceptance:
        parts.append(f"<div><b>éªŒæ”¶æ ‡å‡†</b><pre style='white-space:pre-wrap'>{_escape_md(acceptance)}</pre></div>")
    if fb_reason or fb_required:
        fb_block = ""
        if fb_reason:
            fb_block += f"åŸå› ï¼š{fb_reason}\n"
        if fb_required:
            fb_block += f"è¦æ±‚ï¼š\n{fb_required}\n"
        parts.append(
            "<div><b>å®¡é˜…/å¤±è´¥åé¦ˆ</b>"
            f"<pre style='white-space:pre-wrap'>{_escape_md(fb_block)}</pre></div>"
        )

    # ç®€è¦å±•ç¤ºåŒä¸€æ­¥éª¤çš„å¤šæ¬¡æ›´æ–°ï¼ˆé‡è¯•/å·¥å…·è¿”å›/å¤šè½®ï¼‰
    if history and len(history) > 1:
        tail = history[-8:]  # åªå±•ç¤ºæœ€è¿‘ 8 æ¡ï¼Œé¿å… UI è¿‡é•¿
        lines = []
        for h in tail:
            a = (h.get("artifact") or {}) if isinstance(h, dict) else {}
            lines.append(
                "#%s node=%s tool_calls=%s attempt=%s" % (
                    h.get("trace_i"),
                    h.get("node"),
                    a.get("tool_calls_count"),
                    a.get("attempt"),
                )
            )
        parts.append(
            "<div><b>æœ¬æ­¥éª¤æ›´æ–°å†å²</b>"
            f"<pre style='white-space:pre-wrap'>{_escape_md(chr(10).join(lines))}</pre></div>"
        )

    if out_disp.strip():
        tip = ""
        if truncated:
            tip = f"ï¼ˆå·²æˆªæ–­ï¼šæ˜¾ç¤ºå‰ {max_output_chars} å­— / å…± {out_full_len} å­—ï¼‰"
        parts.append(
            "<div><b>äº§ç‰©è¾“å‡º</b> "
            f"<span style='color: #666;'>{_escape_md(tip)}</span>"
            f"<pre style='max-height: 280px; overflow:auto; white-space:pre-wrap'>{_escape_md(out_disp)}</pre></div>"
        )
    else:
        parts.append("<div><b>äº§ç‰©è¾“å‡º</b><div style='color:#666'>ï¼ˆæš‚æ— è¾“å‡ºï¼‰</div></div>")

    parts.append("</div>")
    parts.append("</details>")
    return "\n".join(parts)


@st.cache_resource
def get_graph():
    # Initialize tools via MCP
    from llm import init_mcp_tools
    
    # We use asyncio.run to block until tools are loaded, as this is a sync cached function
    # Note: init_mcp_tools now uses temporary connections internally, so no manual cleanup needed here.
    tool_map = asyncio.run(init_mcp_tools())
    
    intent_llm, planner_llm, agent_llm, reflector_llm, responder_llm = build_llm()
    return build_reflection_multi_agent_graph(
        intent_llm, planner_llm, agent_llm, reflector_llm, responder_llm=responder_llm,
        tool_map=tool_map
    )


def get_thread_id():
    if "thread_id" not in st.session_state:
        st.session_state.thread_id = f"thread-{uuid.uuid4()}"
    return st.session_state.thread_id


def render_steps_panel(session_state, traces_count, placeholders):
    """æ¸²æŸ“å³ä¾§æ­¥éª¤é¢æ¿ - ä»…åœ¨æœ‰ traces æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""

    if traces_count == 0:
        placeholders["progress"].info("æš‚æ— æ‰§è¡Œè®°å½•")
        placeholders["current_step"].info("ç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        placeholders["steps_detail"].info("ç­‰å¾…å¼€å§‹æ‰§è¡Œ...")
        return

    # ä» traces ä¸­é‡å»ºå½“å‰çŠ¶æ€
    current_state = reconstruct_state_from_traces(session_state.traces)
    current_plan = current_state["plan"]
    current_step_idx = current_state["step_idx"]
    artifacts_by_step = current_state["artifacts"]
    artifacts_hist = current_state.get("artifact_history") or {}
    step_failures = current_state["step_failures"]
    no_progress = current_state["no_progress"]
    executed_steps = current_state.get("executed_steps") or []

    # æ˜¾ç¤ºæ•´ä½“è¿›åº¦
    if current_plan and "steps" in current_plan:
        steps_norm = _normalize_steps(current_plan.get("steps"))
        total_steps = len(steps_norm)
        completed_steps = len(executed_steps) if executed_steps else len(artifacts_by_step)

        if total_steps > 0:
            progress = completed_steps / total_steps
            placeholders["progress"].progress(min(progress, 1.0))
            placeholders["progress"].caption(
                f"è®¡åˆ’æ­¥éª¤ï¼šå·²å®Œæˆ {completed_steps}/{total_steps} | æ‰§è¡Œäº‹ä»¶ï¼š{traces_count}"
            )

        # æ˜¾ç¤ºå½“å‰æ­£åœ¨æ‰§è¡Œçš„æ­¥éª¤
        if 0 <= current_step_idx < total_steps:
            current_step = steps_norm[current_step_idx]
            step_id = current_step.get("id", f"step{current_step_idx+1}")
            step_info = f"**å½“å‰æ­¥éª¤: {step_id}**\n\n**è§’è‰²:** {current_step.get('agent', 'æœªçŸ¥')}\n\n**ä»»åŠ¡:** {current_step.get('task', '')}"

            if step_id in executed_steps:
                step_info += "\n\nâœ“ æœ¬æ­¥éª¤å·²å®Œæˆ"
            else:
                failure_count = step_failures.get(step_id, 0)
                if failure_count > 0:
                    step_info += f"\n\nâš ï¸ æœ¬æ­¥éª¤å·²é‡è¯• {failure_count} æ¬¡"
                elif no_progress.get(step_id, False):
                    step_info += "\n\nâŒ æœ¬æ­¥éª¤æ— è¿›å±•"
                else:
                    step_info += "\n\nâ³ æœ¬æ­¥éª¤æ­£åœ¨æ‰§è¡Œä¸­..."

            # é¢å¤–ï¼šæ˜¾ç¤ºè¯¥æ­¥éª¤çš„æ›´æ–°æ¬¡æ•°ï¼ˆåŒä¸€ step å¯èƒ½å¤šè½®/å¤šæ¬¡ tool_returnï¼‰
            hist_n = len(artifacts_hist.get(step_id) or [])
            if hist_n:
                step_info += f"\n\nï¼ˆæœ¬æ­¥éª¤å·²äº§ç”Ÿ {hist_n} æ¬¡æ›´æ–°ï¼‰"

            placeholders["current_step"].markdown(step_info)
        else:
            placeholders["current_step"].info("æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ")

        # æ­¥éª¤è¯¦æƒ…æ˜¾ç¤ºï¼ˆåŒ…å«æ¯ä¸ª step çš„ task/acceptance/output/é‡è¯•/å·¥å…·ç»Ÿè®¡ï¼‰
        max_steps_show = 50
        detail_blocks = []
        last_feedback = current_state.get("last_feedback", {}) or {}
        for i, step in enumerate(steps_norm[:max_steps_show]):
            step_id = step.get("id", f"step{i+1}")
            if step_id in executed_steps:
                status = "âœ“"
            elif i == current_step_idx:
                status = "ğŸ”„"
            elif i < current_step_idx:
                status = "â³"
            else:
                status = "â³"

            artifact = artifacts_by_step.get(step_id, {})
            detail_blocks.append(
                _render_step_details_html(
                    step_idx=i,
                    step=step,
                    artifact=artifact,
                    history=artifacts_hist.get(step_id) or [],
                    status=status,
                    failure_count=int(step_failures.get(step_id, 0) or 0),
                    no_progress=bool(no_progress.get(step_id, False)),
                    feedback=last_feedback.get(step_id, {}) or {},
                )
            )

        if len(steps_norm) > max_steps_show:
            detail_blocks.append(
                f"<div style='color:#666'>ï¼ˆä»…å±•ç¤ºå‰ {max_steps_show} ä¸ªæ­¥éª¤ï¼›å½“å‰è®¡åˆ’å…± {len(steps_norm)} ä¸ªæ­¥éª¤ï¼‰</div>"
            )

        placeholders["steps_detail"].markdown(
            "\n".join(detail_blocks), unsafe_allow_html=True
        )
    else:
        placeholders["progress"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")
        placeholders["current_step"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")
        placeholders["steps_detail"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")


async def run_graph_stream(graph, user_text: str, thread_id: str):
    # åªä¼ æœ¬è½®å¢é‡è¾“å…¥ï¼Œé¿å…è¦†ç›–å†å²
    init_state = {"user_request": user_text}
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 1000}

    async for upd in graph.astream(init_state, config=config, stream_mode="updates"):
        # upd: { node_name: patch }
        yield upd


def setup_logging(level=logging.INFO):
    print(f"Setting up logging with level {level}")
    # ã€å…³é”®ã€‘åœ¨è¿™é‡Œç»Ÿä¸€é…ç½®
    logging.basicConfig(
        level=level,
        format='time="%(asctime)s" level=%(levelname)s event=%(message)s',
        handlers=[
            logging.FileHandler("app.log"),  # è¾“å‡ºåˆ°æ–‡ä»¶
            # logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        ],
        force=True,
    )


def main():
    workspace = utils.env("WORKSPACE.ROOT")
    if workspace is None or workspace == "":
        raise SystemExit("WORKSPACE.ROOT is not set")

    skills_dir = utils.env("SKILLS.DIR")
    if skills_dir is None or skills_dir == "":
        raise SystemExit("SKILLS.DIR is not set")

    intent_provider = utils.env("INTENT.PROVIDER")
    if intent_provider is None or intent_provider == "":
        raise SystemExit("INTENT.PROVIDER is not set")

    intent_model = utils.env("INTENT.MODEL")
    if intent_model is None or intent_model == "":
        raise SystemExit("INTENT.MODEL is not set")

    if not utils.check_llm_provider(intent_provider, intent_model):
        raise SystemExit(
            f"INTENT.PROVIDER={intent_provider} INTENT.MODEL={intent_model} is not configured"
        )

    planner_provider = utils.env("PLANNER.PROVIDER")
    if planner_provider is None or planner_provider == "":
        raise SystemExit("PLANNER.PROVIDER is not set")

    planner_model = utils.env("PLANNER.MODEL")
    if planner_model is None or planner_model == "":
        raise SystemExit("PLANNER.MODEL is not set")

    if not utils.check_llm_provider(planner_provider, planner_model):
        raise SystemExit(
            f"PLANNER.PROVIDER={planner_provider} PLANNER.MODEL={planner_model} is not configured"
        )

    agent_provider = utils.env("AGENT.PROVIDER")
    if agent_provider is None or agent_provider == "":
        raise SystemExit("AGENT.PROVIDER is not set")

    agent_model = utils.env("AGENT.MODEL")
    if agent_model is None or agent_model == "":
        raise SystemExit("AGENT.MODEL is not set")
    if not utils.check_llm_provider(agent_provider, agent_model):
        raise SystemExit(
            f"AGENT.PROVIDER={agent_provider} AGENT.MODEL={agent_model} is not configured"
        )

    search_backend = utils.env("SEARCH.BACKEND")
    if search_backend is None or search_backend == "":
        raise SystemExit("SEARCH.BACKEND is not set")

    # è®¾ç½®æ—¥å¿—
    log_level = utils.env("LOG.LEVEL", "INFO")
    if log_level.upper() == "DEBUG":
        setup_logging(logging.DEBUG)
    elif log_level.upper() == "INFO":
        setup_logging(logging.INFO)
    elif log_level.upper() == "WARNING":
        setup_logging(logging.WARNING)
    elif log_level.upper() == "ERROR":
        setup_logging(logging.ERROR)
    elif log_level.upper() == "CRITICAL":
        setup_logging(logging.CRITICAL)
    else:
        setup_logging(logging.INFO)

    logger = logging.getLogger(__name__)

    logger.info(
        f"[boot] WORKSPACE.ROOT={workspace} SKILLS.DIR={skills_dir} TOOL_REGISTRY={TOOL_REGISTRY} LOG_LEVEL={log_level}"
    )
    st.set_page_config(page_title="Agent Chat UI", layout="wide")

    # å·¦ï¼šèŠå¤©ï¼›å³ï¼šæ­¥éª¤/æ—¥å¿—
    col_chat, col_steps = st.columns([2, 1], gap="large")

    # å³ä¾§è¿›åº¦é¢æ¿çš„å ä½ç¬¦å­—å…¸
    placeholders = {}

    if "messages" not in st.session_state:
        st.session_state.messages = (
            []
        )  # [{"role": "user"/"assistant", "content": "..."}]
    if "traces" not in st.session_state:
        st.session_state.traces = []  # [{"node":..., "patch":...}, ...]

    graph = get_graph()
    thread_id = get_thread_id()

    with col_steps:
        st.subheader("ä»»åŠ¡ç›‘æ§")
        st.caption(f"thread_id = {thread_id}")

        # ä½¿ç”¨ Tabs åˆ†ç»„æ˜¾ç¤º
        tab_progress, tab_visual, tab_logs, tab_token = st.tabs(["æ‰§è¡Œè¿›åº¦", "æ—¶åºå›¾", "ç³»ç»Ÿæ—¥å¿—", "Tokenç»Ÿè®¡"])

        with tab_progress:
            # åˆ›å»ºå ä½ç¬¦å¹¶å­˜å‚¨åˆ°å­—å…¸
            placeholders["progress"] = st.empty()
            placeholders["current_step"] = st.empty()
            placeholders["steps_detail"] = st.empty()
        
        with tab_visual:
            placeholders["mermaid"] = st.empty()
            
        with tab_logs:
            placeholders["logs"] = st.empty()
            
        with tab_token:
            placeholders["token"] = st.empty()

        if "ui_logs" not in st.session_state:
            st.session_state.ui_logs = []

        traces_count = len(st.session_state.traces)
        render_steps_panel(st.session_state, traces_count, placeholders)


        def update_token_display():
            if "total_token_usage" in st.session_state:
                usage = st.session_state.total_token_usage
                token_info = f"æç¤º: {usage.get('prompt_tokens', 0)} | å®Œæˆ: {usage.get('completion_tokens', 0)} | æ€»è®¡: {usage.get('total_tokens', 0)}"
                placeholders["token"].markdown(token_info)
            else:
                placeholders["token"].text("æš‚æ— æ•°æ®")

        update_token_display()
        
        # åˆå§‹æ¸²æŸ“æ—¶åºå›¾
        if traces_count > 0:
            mermaid_code = generate_mermaid_sequence(st.session_state.traces)
            with placeholders["mermaid"]:
                render_mermaid(mermaid_code, height=500)
        else:
            placeholders["mermaid"].info("ç­‰å¾…æ‰§è¡Œ...")

    with col_chat:
        st.title("Chat")
        # å†å²æ¶ˆæ¯å›æ”¾
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        user_text = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜â€¦")
        if user_text:
            st.session_state.messages.append({"role": "user", "content": user_text})
            with st.chat_message("user"):
                st.markdown(user_text)

            # assistant æµå¼åŒºåŸŸ
            with st.chat_message("assistant"):
                placeholder = st.empty()
                acc = ""

                async def _drive():
                    nonlocal acc
                    last_refresh = 0.0

                    try:
                        async for upd in run_graph_stream(graph, user_text, thread_id):
                            mem_snapshot("memory metric")
                            # è°ƒè¯•ï¼šè®°å½•æ›´æ–°ç»“æ„
                            logger.debug(f"æ›´æ–°: {list(upd.keys())}")
                            for node, patch in upd.items():
                                st.session_state.traces.append(
                                    {"node": node, "patch": patch}
                                )
                                # è°ƒè¯•ï¼šè®°å½•èŠ‚ç‚¹å’Œè¡¥ä¸é”®
                                if patch and isinstance(patch, dict):
                                    logger.debug(
                                        f"èŠ‚ç‚¹ {node} è¡¥ä¸é”®: {list(patch.keys())}"
                                    )

                                # æå–å¹¶ç´¯è®¡tokenä½¿ç”¨é‡
                                if patch and isinstance(patch, dict):
                                    current_usage = None
                                    # æ–¹æ³•1ï¼šç›´æ¥æ£€æŸ¥patchä¸­æ˜¯å¦æœ‰usage_metadata
                                    if "usage_metadata" in patch:
                                        usage = patch["usage_metadata"]
                                        if isinstance(usage, dict):
                                            current_usage = usage
                                    # æ–¹æ³•2ï¼šä»messagesä¸­æå–usage_metadata
                                    elif "messages" in patch:
                                        messages = patch["messages"]
                                        if messages:
                                            last_msg = messages[-1]
                                            # ä»æ¶ˆæ¯ä¸­æå–usage_metadata
                                            usage = getattr(
                                                last_msg, "usage_metadata", None
                                            )
                                            if usage and isinstance(usage, dict):
                                                current_usage = usage

                                    if current_usage:
                                        # åˆå§‹åŒ–ç´¯è®¡ä½¿ç”¨é‡
                                        if "total_token_usage" not in st.session_state:
                                            st.session_state.total_token_usage = {
                                                "prompt_tokens": 0,
                                                "completion_tokens": 0,
                                                "total_tokens": 0,
                                            }
                                        # åˆå§‹åŒ–å†å²è®°å½•
                                        if "token_history" not in st.session_state:
                                            st.session_state.token_history = []

                                        # ç´¯è®¡tokenä½¿ç”¨é‡
                                        st.session_state.total_token_usage[
                                            "prompt_tokens"
                                        ] += current_usage.get("prompt_tokens", 0)
                                        st.session_state.total_token_usage[
                                            "completion_tokens"
                                        ] += current_usage.get("completion_tokens", 0)
                                        st.session_state.total_token_usage[
                                            "total_tokens"
                                        ] += current_usage.get("total_tokens", 0)

                                        # æ·»åŠ åˆ°å†å²è®°å½•
                                        history_record = {
                                            "node": node,
                                            "prompt_tokens": current_usage.get(
                                                "prompt_tokens", 0
                                            ),
                                            "completion_tokens": current_usage.get(
                                                "completion_tokens", 0
                                            ),
                                            "total_tokens": current_usage.get(
                                                "total_tokens", 0
                                            ),
                                            "timestamp": time.time(),
                                        }
                                        st.session_state.token_history.append(
                                            history_record
                                        )

                                        logger.debug(
                                            f"ç´¯è®¡tokenä½¿ç”¨é‡: {st.session_state.total_token_usage}"
                                        )

                                        # æ›´æ–°tokenæ˜¾ç¤ºå ä½ç¬¦
                                        if placeholders and "token" in placeholders:
                                            usage = st.session_state.total_token_usage
                                            display_text = f"ç´¯è®¡æç¤ºtoken: {usage.get('prompt_tokens', 0)}\n"
                                            display_text += f"ç´¯è®¡å®Œæˆtoken: {usage.get('completion_tokens', 0)}\n"
                                            display_text += f"ç´¯è®¡æ€»token: {usage.get('total_tokens', 0)}"

                                            # æ˜¾ç¤ºtokenä½¿ç”¨å†å²
                                            if (
                                                "token_history" in st.session_state
                                                and st.session_state.token_history
                                            ):
                                                display_text += "\n\n**Tokenä½¿ç”¨å†å²:**"
                                                for i, record in enumerate(
                                                    st.session_state.token_history[-10:]
                                                ):  # æ˜¾ç¤ºæœ€è¿‘10æ¡
                                                    display_text += f"\n{i+1}. èŠ‚ç‚¹: {record.get('node', 'æœªçŸ¥')}, "
                                                    display_text += f"æç¤ºtoken: {record.get('prompt_tokens', 0)}, "
                                                    display_text += f"å®Œæˆtoken: {record.get('completion_tokens', 0)}, "
                                                    display_text += f"æ€»token: {record.get('total_tokens', 0)}"

                                            placeholders["token"].markdown(display_text)

                                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘UIæ›´æ–°
                                # âœ… åˆ·æ–°å³ä¾§ï¼ˆå»ºè®®èŠ‚æµï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
                                now = time.time()
                                if now - last_refresh > 0.5:  # æ”¾å®½åˆ·æ–°é¢‘ç‡
                                    render_steps_panel(
                                        st.session_state, len(st.session_state.traces), placeholders
                                    )
                                    
                                    # åˆ·æ–° Mermaid
                                    mermaid_code = generate_mermaid_sequence(st.session_state.traces)
                                    with placeholders["mermaid"]:
                                        render_mermaid(mermaid_code, height=500)
                                        
                                    last_refresh = now

                                # æ³¨æ„ï¼šä¸­é—´èŠ‚ç‚¹ï¼ˆresearcher, solver, writerï¼‰çš„è¾“å‡ºä¸å†æ˜¾ç¤ºåœ¨å·¦ä¾§èŠå¤©åŒºåŸŸ
                                # è¿™äº›ä¿¡æ¯å°†åœ¨å³ä¾§è¿›åº¦é¢æ¿ä¸­æ˜¾ç¤º

                                # å¦‚æœæ˜¯æœ€ç»ˆèŠ‚ç‚¹ï¼ŒæŠŠ final_answer æµå¼å†™å‡ºæ¥
                                if node == "respond":
                                    final = (patch or {}).get("final_answer", "") or ""
                                    if final:
                                        acc = final
                                        placeholder.markdown(acc)

                                st.session_state.ui_logs.append(
                                    f"[{node}] keys={list(patch.keys()) if isinstance(patch, dict) else type(patch)}")
                                st.session_state.ui_logs = st.session_state.ui_logs[-300:]

                                # é¢å¤–ï¼šç»Ÿè®¡å„èŠ‚ç‚¹å‡ºç°æ¬¡æ•°ï¼Œå¸®åŠ©å¯¹é½ app.log çš„â€œæ‰§è¡Œè½®æ¬¡/èŠ‚ç‚¹æ›´å¤šâ€ç°è±¡
                                try:
                                    counts = {}
                                    for t in st.session_state.traces[-2000:]:
                                        n = t.get("node")
                                        if not n:
                                            continue
                                        counts[n] = counts.get(n, 0) + 1
                                    counts_sorted = sorted(counts.items(), key=lambda x: (-x[1], x[0]))
                                    counts_text = ", ".join([f"{k}={v}" for k, v in counts_sorted[:25]])
                                    if len(counts_sorted) > 25:
                                        counts_text += ", ..."
                                except Exception:
                                    counts_text = ""

                                header = "èŠ‚ç‚¹è®¡æ•°ï¼ˆè¿‘ 2000 æ¡ trace èšåˆï¼‰ï¼š\n" + (counts_text or "ï¼ˆæš‚æ— ï¼‰")
                                body = "\n".join(st.session_state.ui_logs)
                                placeholders["logs"].code(header + "\n\næœ€è¿‘ updatesï¼š\n" + body, language="text")

                        return acc
                    finally:
                        # Ensure we cleanup any MCP sessions created in this loop
                        await CLIENT_MANAGER.close_all()

                final_answer = asyncio.run(_drive())

            render_steps_panel(st.session_state, len(st.session_state.traces), placeholders)

            st.session_state.messages.append(
                {"role": "assistant", "content": final_answer}
            )


if __name__ == "__main__":
    main()
