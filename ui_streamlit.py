import asyncio
import logging
import time
import uuid
import streamlit as st
import streamlit.components.v1 as components

import utils

# ä½ ç°æœ‰çš„æ„å»ºå‡½æ•°ï¼ˆæŒ‰ä½ çš„é¡¹ç›®æ”¹ importï¼‰
from llm import build_llm, build_reflection_multi_agent_graph
from tools import TOOL_REGISTRY
from trace_visualizer import generate_mermaid_sequence
from mcp_adapter import CLIENT_MANAGER


def render_mermaid(code: str, height=600):
    """
    ä½¿ç”¨ HTML/JS æ¸²æŸ“ Mermaid å›¾è¡¨
    """
    html_code = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
        <script>
            mermaid.initialize({{ startOnLoad: true, theme: 'default' }});
        </script>
    </head>
    <body>
        <div class="mermaid">
            {code}
        </div>
    </body>
    </html>
    """
    components.html(html_code, height=height, scrolling=True)


def reconstruct_state_from_traces(traces):
    """ä» traces ä¸­é‡å»ºå½“å‰çŠ¶æ€"""
    current_state = {
        "plan": None,
        "step_idx": 0,
        "artifacts": {},
        "step_failures": {},
        "step_tool_stats": {},
        "no_progress": {},
        "last_feedback": {},
        "iter_count": 0,
        "done": False,
    }

    # æŒ‰æ—¶é—´é¡ºåºå¤„ç† tracesï¼ˆä»æ—§åˆ°æ–°ï¼‰ï¼Œæ¨¡æ‹ŸçŠ¶æ€æ›´æ–°
    for trace in traces:
        patch = trace.get("patch") or {}
        node = trace.get("node", "")

        # è°ƒè¯•ï¼šæ‰“å°èŠ‚ç‚¹å’Œè¡¥ä¸ç»“æ„
        # print(f"èŠ‚ç‚¹: {node}, è¡¥ä¸é”®: {list(patch.keys())}")

        # åº”ç”¨è¡¥ä¸åˆ°å½“å‰çŠ¶æ€
        # LangGraph çš„è¡¥ä¸å¯èƒ½æ˜¯åµŒå¥—çš„ï¼Œä¾‹å¦‚ {"plan": {...}} æˆ– {"step_idx": 1}
        # ä½†ä¹Ÿå¯èƒ½æ˜¯æ›´å¤æ‚çš„ç»“æ„ï¼Œä¾‹å¦‚ {"artifacts": {"step1": {...}}}
        for key, value in patch.items():
            if key in ["plan", "step_idx", "iter_count", "done"]:
                current_state[key] = value
            elif key in [
                "artifacts",
                "step_failures",
                "step_tool_stats",
                "no_progress",
                "last_feedback",
            ]:
                if isinstance(value, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶æ›´æ–°
                    if key == "artifacts":
                        # artifacts å¯èƒ½åŒ…å«å¤šä¸ªæ­¥éª¤çš„è¾“å‡ºï¼Œéœ€è¦åˆå¹¶
                        for step_id, artifact in value.items():
                            current_state[key][step_id] = artifact
                    else:
                        current_state[key].update(value)

    return current_state


@st.cache_resource
def get_graph():
    # Initialize tools via MCP
    from llm import init_mcp_tools
    
    # We use asyncio.run to block until tools are loaded, as this is a sync cached function
    # Note: init_mcp_tools now uses temporary connections internally, so no manual cleanup needed here.
    tool_map = asyncio.run(init_mcp_tools())
    
    intent_llm, planner_llm, agent_llm, reflector_llm, responder_llm = build_llm()
    return build_reflection_multi_agent_graph(
        intent_llm, planner_llm, agent_llm, reflector_llm, responder_llm=responder_llm,
        tool_map=tool_map
    )


def get_thread_id():
    if "thread_id" not in st.session_state:
        st.session_state.thread_id = f"thread-{uuid.uuid4()}"
    return st.session_state.thread_id


def render_steps_panel(session_state, traces_count, placeholders):
    """æ¸²æŸ“å³ä¾§æ­¥éª¤é¢æ¿ - ä»…åœ¨æœ‰ traces æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯"""

    if traces_count == 0:
        placeholders["progress"].info("æš‚æ— æ‰§è¡Œè®°å½•")
        placeholders["current_step"].info("ç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        placeholders["steps_detail"].info("ç­‰å¾…å¼€å§‹æ‰§è¡Œ...")
        return

    # ä» traces ä¸­é‡å»ºå½“å‰çŠ¶æ€
    current_state = reconstruct_state_from_traces(session_state.traces)
    current_plan = current_state["plan"]
    current_step_idx = current_state["step_idx"]
    artifacts_by_step = current_state["artifacts"]
    step_failures = current_state["step_failures"]
    no_progress = current_state["no_progress"]

    # æ˜¾ç¤ºæ•´ä½“è¿›åº¦
    if current_plan and "steps" in current_plan:
        total_steps = len(current_plan["steps"])
        completed_steps = len(artifacts_by_step)

        if total_steps > 0:
            progress = completed_steps / total_steps
            placeholders["progress"].progress(min(progress, 1.0))
            placeholders["progress"].caption(
                f"å·²å®Œæˆ {completed_steps}/{total_steps} ä¸ªæ­¥éª¤"
            )

        # æ˜¾ç¤ºå½“å‰æ­£åœ¨æ‰§è¡Œçš„æ­¥éª¤
        if 0 <= current_step_idx < total_steps:
            current_step = current_plan["steps"][current_step_idx]
            step_id = current_step.get("id", f"step{current_step_idx+1}")
            step_info = f"**å½“å‰æ­¥éª¤: {step_id}**\n\n**è§’è‰²:** {current_step.get('agent', 'æœªçŸ¥')}\n\n**ä»»åŠ¡:** {current_step.get('task', '')}"

            if step_id in artifacts_by_step:
                step_info += "\n\nâœ“ æœ¬æ­¥éª¤å·²å®Œæˆ"
            else:
                failure_count = step_failures.get(step_id, 0)
                if failure_count > 0:
                    step_info += f"\n\nâš ï¸ æœ¬æ­¥éª¤å·²é‡è¯• {failure_count} æ¬¡"
                elif no_progress.get(step_id, False):
                    step_info += "\n\nâŒ æœ¬æ­¥éª¤æ— è¿›å±•"
                else:
                    step_info += "\n\nâ³ æœ¬æ­¥éª¤æ­£åœ¨æ‰§è¡Œä¸­..."

            placeholders["current_step"].markdown(step_info)
        else:
            placeholders["current_step"].info("æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ")

        # ç®€åŒ–æ­¥éª¤è¯¦æƒ…æ˜¾ç¤º
        steps_detail = []
        for i, step in enumerate(current_plan["steps"][:20]):  # é™åˆ¶æ˜¾ç¤ºå‰20ä¸ªæ­¥éª¤
            step_id = step.get("id", f"step{i+1}")
            step_agent = step.get("agent", "æœªçŸ¥")

            if i < current_step_idx:
                status = "âœ“"
            elif i == current_step_idx:
                if step_id in artifacts_by_step:
                    status = "âœ“"
                else:
                    status = "ğŸ”„"
            else:
                status = "â³"

            steps_detail.append(f"{status} {step_id} ({step_agent})")

        if len(current_plan["steps"]) > 20:
            steps_detail.append(f"... å…± {len(current_plan['steps'])} ä¸ªæ­¥éª¤")

        placeholders["steps_detail"].markdown("\n".join(steps_detail))
    else:
        placeholders["progress"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")
        placeholders["current_step"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")
        placeholders["steps_detail"].info("ç­‰å¾…è®¡åˆ’ç”Ÿæˆ...")


async def run_graph_stream(graph, user_text: str, thread_id: str):
    # åªä¼ æœ¬è½®å¢é‡è¾“å…¥ï¼Œé¿å…è¦†ç›–å†å²
    init_state = {"user_request": user_text}
    config = {"configurable": {"thread_id": thread_id}, "recursion_limit": 1000}

    async for upd in graph.astream(init_state, config=config, stream_mode="updates"):
        # upd: { node_name: patch }
        yield upd


def setup_logging(level=logging.INFO):
    print(f"Setting up logging with level {level}")
    # ã€å…³é”®ã€‘åœ¨è¿™é‡Œç»Ÿä¸€é…ç½®
    logging.basicConfig(
        level=level,
        format='time="%(asctime)s" level=%(levelname)s event=%(message)s',
        handlers=[
            logging.FileHandler("app.log"),  # è¾“å‡ºåˆ°æ–‡ä»¶
            # logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        ],
        force=True,
    )


def main():
    workspace = utils.env("WORKSPACE.ROOT")
    if workspace is None or workspace == "":
        raise SystemExit("WORKSPACE.ROOT is not set")

    skills_dir = utils.env("SKILLS.DIR")
    if skills_dir is None or skills_dir == "":
        raise SystemExit("SKILLS.DIR is not set")

    intent_provider = utils.env("INTENT.PROVIDER")
    if intent_provider is None or intent_provider == "":
        raise SystemExit("INTENT.PROVIDER is not set")

    intent_model = utils.env("INTENT.MODEL")
    if intent_model is None or intent_model == "":
        raise SystemExit("INTENT.MODEL is not set")

    if not utils.check_llm_provider(intent_provider, intent_model):
        raise SystemExit(
            f"INTENT.PROVIDER={intent_provider} INTENT.MODEL={intent_model} is not configured"
        )

    planner_provider = utils.env("PLANNER.PROVIDER")
    if planner_provider is None or planner_provider == "":
        raise SystemExit("PLANNER.PROVIDER is not set")

    planner_model = utils.env("PLANNER.MODEL")
    if planner_model is None or planner_model == "":
        raise SystemExit("PLANNER.MODEL is not set")

    if not utils.check_llm_provider(planner_provider, planner_model):
        raise SystemExit(
            f"PLANNER.PROVIDER={planner_provider} PLANNER.MODEL={planner_model} is not configured"
        )

    agent_provider = utils.env("AGENT.PROVIDER")
    if agent_provider is None or agent_provider == "":
        raise SystemExit("AGENT.PROVIDER is not set")

    agent_model = utils.env("AGENT.MODEL")
    if agent_model is None or agent_model == "":
        raise SystemExit("AGENT.MODEL is not set")
    if not utils.check_llm_provider(agent_provider, agent_model):
        raise SystemExit(
            f"AGENT.PROVIDER={agent_provider} AGENT.MODEL={agent_model} is not configured"
        )

    search_backend = utils.env("SEARCH.BACKEND")
    if search_backend is None or search_backend == "":
        raise SystemExit("SEARCH.BACKEND is not set")

    # è®¾ç½®æ—¥å¿—
    log_level = utils.env("LOG.LEVEL", "INFO")
    if log_level.upper() == "DEBUG":
        setup_logging(logging.DEBUG)
    elif log_level.upper() == "INFO":
        setup_logging(logging.INFO)
    elif log_level.upper() == "WARNING":
        setup_logging(logging.WARNING)
    elif log_level.upper() == "ERROR":
        setup_logging(logging.ERROR)
    elif log_level.upper() == "CRITICAL":
        setup_logging(logging.CRITICAL)
    else:
        setup_logging(logging.INFO)

    logger = logging.getLogger(__name__)

    logger.info(
        f"[boot] WORKSPACE.ROOT={workspace} SKILLS.DIR={skills_dir} TOOL_REGISTRY={TOOL_REGISTRY} LOG_LEVEL={log_level}"
    )
    st.set_page_config(page_title="Agent Chat UI", layout="wide")

    # å·¦ï¼šèŠå¤©ï¼›å³ï¼šæ­¥éª¤/æ—¥å¿—
    col_chat, col_steps = st.columns([2, 1], gap="large")

    # å³ä¾§è¿›åº¦é¢æ¿çš„å ä½ç¬¦å­—å…¸
    placeholders = {}

    if "messages" not in st.session_state:
        st.session_state.messages = (
            []
        )  # [{"role": "user"/"assistant", "content": "..."}]
    if "traces" not in st.session_state:
        st.session_state.traces = []  # [{"node":..., "patch":...}, ...]

    graph = get_graph()
    thread_id = get_thread_id()

    with col_steps:
        st.subheader("ä»»åŠ¡ç›‘æ§")
        st.caption(f"thread_id = {thread_id}")

        # ä½¿ç”¨ Tabs åˆ†ç»„æ˜¾ç¤º
        tab_progress, tab_visual, tab_logs, tab_token = st.tabs(["æ‰§è¡Œè¿›åº¦", "æ—¶åºå›¾", "ç³»ç»Ÿæ—¥å¿—", "Tokenç»Ÿè®¡"])

        with tab_progress:
            # åˆ›å»ºå ä½ç¬¦å¹¶å­˜å‚¨åˆ°å­—å…¸
            placeholders["progress"] = st.empty()
            placeholders["current_step"] = st.empty()
            placeholders["steps_detail"] = st.empty()
        
        with tab_visual:
            placeholders["mermaid"] = st.empty()
            
        with tab_logs:
            placeholders["logs"] = st.empty()
            
        with tab_token:
            placeholders["token"] = st.empty()

        if "ui_logs" not in st.session_state:
            st.session_state.ui_logs = []

        traces_count = len(st.session_state.traces)
        render_steps_panel(st.session_state, traces_count, placeholders)


        def update_token_display():
            if "total_token_usage" in st.session_state:
                usage = st.session_state.total_token_usage
                token_info = f"æç¤º: {usage.get('prompt_tokens', 0)} | å®Œæˆ: {usage.get('completion_tokens', 0)} | æ€»è®¡: {usage.get('total_tokens', 0)}"
                placeholders["token"].markdown(token_info)
            else:
                placeholders["token"].text("æš‚æ— æ•°æ®")

        update_token_display()
        
        # åˆå§‹æ¸²æŸ“æ—¶åºå›¾
        if traces_count > 0:
            mermaid_code = generate_mermaid_sequence(st.session_state.traces)
            with placeholders["mermaid"]:
                render_mermaid(mermaid_code, height=500)
        else:
            placeholders["mermaid"].info("ç­‰å¾…æ‰§è¡Œ...")

    with col_chat:
        st.title("Chat")
        # å†å²æ¶ˆæ¯å›æ”¾
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

        user_text = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜â€¦")
        if user_text:
            st.session_state.messages.append({"role": "user", "content": user_text})
            with st.chat_message("user"):
                st.markdown(user_text)

            # assistant æµå¼åŒºåŸŸ
            with st.chat_message("assistant"):
                placeholder = st.empty()
                acc = ""

                async def _drive():
                    nonlocal acc
                    last_refresh = 0.0

                    try:
                        async for upd in run_graph_stream(graph, user_text, thread_id):
                            # è°ƒè¯•ï¼šè®°å½•æ›´æ–°ç»“æ„
                            logger.debug(f"æ›´æ–°: {list(upd.keys())}")
                            for node, patch in upd.items():
                                st.session_state.traces.append(
                                    {"node": node, "patch": patch}
                                )
                                # è°ƒè¯•ï¼šè®°å½•èŠ‚ç‚¹å’Œè¡¥ä¸é”®
                                if patch and isinstance(patch, dict):
                                    logger.debug(
                                        f"èŠ‚ç‚¹ {node} è¡¥ä¸é”®: {list(patch.keys())}"
                                    )

                                # æå–å¹¶ç´¯è®¡tokenä½¿ç”¨é‡
                                if patch and isinstance(patch, dict):
                                    current_usage = None
                                    # æ–¹æ³•1ï¼šç›´æ¥æ£€æŸ¥patchä¸­æ˜¯å¦æœ‰usage_metadata
                                    if "usage_metadata" in patch:
                                        usage = patch["usage_metadata"]
                                        if isinstance(usage, dict):
                                            current_usage = usage
                                    # æ–¹æ³•2ï¼šä»messagesä¸­æå–usage_metadata
                                    elif "messages" in patch:
                                        messages = patch["messages"]
                                        if messages:
                                            last_msg = messages[-1]
                                            # ä»æ¶ˆæ¯ä¸­æå–usage_metadata
                                            usage = getattr(
                                                last_msg, "usage_metadata", None
                                            )
                                            if usage and isinstance(usage, dict):
                                                current_usage = usage

                                    if current_usage:
                                        # åˆå§‹åŒ–ç´¯è®¡ä½¿ç”¨é‡
                                        if "total_token_usage" not in st.session_state:
                                            st.session_state.total_token_usage = {
                                                "prompt_tokens": 0,
                                                "completion_tokens": 0,
                                                "total_tokens": 0,
                                            }
                                        # åˆå§‹åŒ–å†å²è®°å½•
                                        if "token_history" not in st.session_state:
                                            st.session_state.token_history = []

                                        # ç´¯è®¡tokenä½¿ç”¨é‡
                                        st.session_state.total_token_usage[
                                            "prompt_tokens"
                                        ] += current_usage.get("prompt_tokens", 0)
                                        st.session_state.total_token_usage[
                                            "completion_tokens"
                                        ] += current_usage.get("completion_tokens", 0)
                                        st.session_state.total_token_usage[
                                            "total_tokens"
                                        ] += current_usage.get("total_tokens", 0)

                                        # æ·»åŠ åˆ°å†å²è®°å½•
                                        history_record = {
                                            "node": node,
                                            "prompt_tokens": current_usage.get(
                                                "prompt_tokens", 0
                                            ),
                                            "completion_tokens": current_usage.get(
                                                "completion_tokens", 0
                                            ),
                                            "total_tokens": current_usage.get(
                                                "total_tokens", 0
                                            ),
                                            "timestamp": time.time(),
                                        }
                                        st.session_state.token_history.append(
                                            history_record
                                        )

                                        logger.debug(
                                            f"ç´¯è®¡tokenä½¿ç”¨é‡: {st.session_state.total_token_usage}"
                                        )

                                        # æ›´æ–°tokenæ˜¾ç¤ºå ä½ç¬¦
                                        if placeholders and "token" in placeholders:
                                            usage = st.session_state.total_token_usage
                                            display_text = f"ç´¯è®¡æç¤ºtoken: {usage.get('prompt_tokens', 0)}\n"
                                            display_text += f"ç´¯è®¡å®Œæˆtoken: {usage.get('completion_tokens', 0)}\n"
                                            display_text += f"ç´¯è®¡æ€»token: {usage.get('total_tokens', 0)}"

                                            # æ˜¾ç¤ºtokenä½¿ç”¨å†å²
                                            if (
                                                "token_history" in st.session_state
                                                and st.session_state.token_history
                                            ):
                                                display_text += "\n\n**Tokenä½¿ç”¨å†å²:**"
                                                for i, record in enumerate(
                                                    st.session_state.token_history[-10:]
                                                ):  # æ˜¾ç¤ºæœ€è¿‘10æ¡
                                                    display_text += f"\n{i+1}. èŠ‚ç‚¹: {record.get('node', 'æœªçŸ¥')}, "
                                                    display_text += f"æç¤ºtoken: {record.get('prompt_tokens', 0)}, "
                                                    display_text += f"å®Œæˆtoken: {record.get('completion_tokens', 0)}, "
                                                    display_text += f"æ€»token: {record.get('total_tokens', 0)}"

                                            placeholders["token"].markdown(display_text)

                                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘UIæ›´æ–°
                                # âœ… åˆ·æ–°å³ä¾§ï¼ˆå»ºè®®èŠ‚æµï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
                                now = time.time()
                                if now - last_refresh > 0.5:  # æ”¾å®½åˆ·æ–°é¢‘ç‡
                                    render_steps_panel(
                                        st.session_state, len(st.session_state.traces), placeholders
                                    )
                                    
                                    # åˆ·æ–° Mermaid
                                    mermaid_code = generate_mermaid_sequence(st.session_state.traces)
                                    with placeholders["mermaid"]:
                                        render_mermaid(mermaid_code, height=500)
                                        
                                    last_refresh = now

                                # æ³¨æ„ï¼šä¸­é—´èŠ‚ç‚¹ï¼ˆresearcher, solver, writerï¼‰çš„è¾“å‡ºä¸å†æ˜¾ç¤ºåœ¨å·¦ä¾§èŠå¤©åŒºåŸŸ
                                # è¿™äº›ä¿¡æ¯å°†åœ¨å³ä¾§è¿›åº¦é¢æ¿ä¸­æ˜¾ç¤º

                                # å¦‚æœæ˜¯æœ€ç»ˆèŠ‚ç‚¹ï¼ŒæŠŠ final_answer æµå¼å†™å‡ºæ¥
                                if node == "respond":
                                    final = (patch or {}).get("final_answer", "") or ""
                                    if final:
                                        acc = final
                                        placeholder.markdown(acc)

                                st.session_state.ui_logs.append(
                                    f"[{node}] keys={list(patch.keys()) if isinstance(patch, dict) else type(patch)}")
                                st.session_state.ui_logs = st.session_state.ui_logs[-300:]
                                placeholders["logs"].code("\n".join(st.session_state.ui_logs), language="text")

                        return acc
                    finally:
                        # Ensure we cleanup any MCP sessions created in this loop
                        await CLIENT_MANAGER.close_all()

                final_answer = asyncio.run(_drive())

            render_steps_panel(st.session_state, len(st.session_state.traces), placeholders)

            st.session_state.messages.append(
                {"role": "assistant", "content": final_answer}
            )


if __name__ == "__main__":
    main()
